{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9M10MxpUpTVu",
        "colab_type": "code",
        "outputId": "30651e7e-ebcc-4678-e648-abae86696d07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "source": [
        "#!pip3 install gensim\n",
        "#!pip3 install wget\n",
        "#https://stackoverflow.com/questions/47744131/colaboratory-can-i-access-to-my-google-drive-folder-and-file\n",
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "(Reading database ... 130912 files and directories currently installed.)\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.7.3-0ubuntu3~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.3-0ubuntu3~ubuntu18.04.1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.3-0ubuntu3~ubuntu18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gI0PgQ24p6dI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "import collections\n",
        "import pickle\n",
        "import numpy as np\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "from gensim.test.utils import get_tmpfile\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtbhSHBnqDow",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#default path for the folder inside google drive\n",
        "default_path = \"drive/Colab Notebooks/\"\n",
        "\n",
        "#path for getting training data\n",
        "train_article_path = default_path + \"sumdata/train/train.article.txt\" \n",
        "\n",
        "#path for training text output (headline)\n",
        "train_title_path   = default_path + \"sumdata/train/train.title.txt\"\n",
        "\n",
        "#path for validation text (article)\n",
        "valid_article_path = default_path + \"sumdata/train/valid.article.filter.txt\"\n",
        "\n",
        "#path for validation text output(headline)\n",
        "valid_title_path   = default_path + \"sumdata/train/valid.title.filter.txt\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpRb2wO8r_DW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_str(sentence):\n",
        "    sentence = re.sub(\"[#.]+\", \"#\", sentence)\n",
        "    return sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjFSrWTrsuYY",
        "colab_type": "code",
        "outputId": "61d60640-378d-42f1-998f-1013c11cb499",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#print(clean_str(\"#####Prabha#.....\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#Prabha#\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJ1KYZq0s0hn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_text_list(data_path, toy):\n",
        "    with open (data_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        if not toy:\n",
        "            return [clean_str(x.strip()) for x in f.readlines()][:200000]\n",
        "        else:\n",
        "            return [clean_str(x.strip()) for x in f.readlines()][:50]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfumJmIbti2Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_dict(step, toy=False):\n",
        "    if step == \"train\":\n",
        "        train_article_list = get_text_list(train_article_path, toy)\n",
        "        train_title_list = get_text_list(train_title_path, toy)\n",
        "\n",
        "        words = list()\n",
        "        for sentence in train_article_list + train_title_list:\n",
        "            for word in word_tokenize(sentence):\n",
        "                words.append(word)\n",
        "\n",
        "        word_counter = collections.Counter(words).most_common()\n",
        "        #print(word_counter)\n",
        "        # ('#', 363119), ('the', 307009), (',', 204614), ('to', 202066) ouuput \n",
        "        word_dict = dict()\n",
        "        word_dict[\"<padding>\"] = 0\n",
        "        word_dict[\"<unk>\"] = 1\n",
        "        word_dict[\"<s>\"] = 2\n",
        "        word_dict[\"</s>\"] = 3\n",
        "        # we are creating the word to int dictionary \n",
        "        for word, _ in word_counter:\n",
        "            word_dict[word] = len(word_dict) \n",
        "\n",
        "        with open(default_path + \"word_dict.pickle\", \"wb\") as f:\n",
        "            pickle.dump(word_dict, f)\n",
        "\n",
        "    elif step == \"valid\":\n",
        "        with open(default_path + \"word_dict.pickle\", \"rb\") as f:\n",
        "            word_dict = pickle.load(f)\n",
        "\n",
        "    reversed_dict = dict(zip(word_dict.values(), word_dict.keys()))\n",
        "\n",
        "    article_max_len = 50\n",
        "    summary_max_len = 15\n",
        "\n",
        "    return word_dict, reversed_dict, article_max_len, summary_max_len"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iN0lwboRuCHx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#a ,b , c, d = build_dict(\"train\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wykTimVovgfb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_dataset(step, word_dict, article_max_len, summary_max_len, toy=False):\n",
        "    if step == \"train\":\n",
        "        article_list = get_text_list(train_article_path, toy)\n",
        "        title_list = get_text_list(train_title_path, toy)\n",
        "    elif step == \"valid\":\n",
        "        article_list = get_text_list(valid_article_path, toy)\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    x = [word_tokenize(d) for d in article_list]\n",
        "    x = [[word_dict.get(w, word_dict[\"<unk>\"]) for w in d] for d in x]\n",
        "    print(\"Hi Prabha, here you are\")\n",
        "    print(x)\n",
        "    x = [d[:article_max_len] for d in x]\n",
        "    x = [d + (article_max_len - len(d)) * [word_dict[\"<padding>\"]] for d in x]\n",
        "    \n",
        "    if step == \"valid\":\n",
        "        return x\n",
        "    else:        \n",
        "        y = [word_tokenize(d) for d in title_list]\n",
        "        y = [[word_dict.get(w, word_dict[\"<unk>\"]) for w in d] for d in y]\n",
        "        y = [d[:(summary_max_len - 1)] for d in y]\n",
        "        return x, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kg7u6jDc0kUL",
        "colab_type": "code",
        "outputId": "40665df6-9343-4ae2-93af-f57161635884",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "#x = build_dataset(\"train\", a, 50, 15)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PH2We3VF0uEl",
        "colab_type": "code",
        "outputId": "4c43cf4c-b038-4a72-9d18-eac540cce508",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "print(\"Building dictionary...\")\n",
        "word_dict, reversed_dict, article_max_len, summary_max_len = build_dict(\"train\", False)\n",
        "print(\"Loading training dataset...\")\n",
        "train_x, train_y = build_dataset(\"train\", word_dict, article_max_len, summary_max_len, False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building dictionary...\n",
            "Loading training dataset...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pG1c7YadLtVK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch_iter(inputs, outputs, batch_size, num_epochs):\n",
        "    inputs = np.array(inputs)\n",
        "    outputs = np.array(outputs)\n",
        "\n",
        "    num_batches_per_epoch = (len(inputs) - 1) // batch_size + 1\n",
        "    for epoch in range(num_epochs):\n",
        "        for batch_num in range(num_batches_per_epoch):\n",
        "            start_index = batch_num * batch_size\n",
        "            end_index = min((batch_num + 1) * batch_size, len(inputs))\n",
        "            yield inputs[start_index:end_index], outputs[start_index:end_index]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3-5o6yySKqv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_init_embedding(reversed_dict, embedding_size):\n",
        "    #glove_file = default_path + \"glove/glove.6B.300d.txt\"\n",
        "    #word2vec_file = get_tmpfile(default_path + \"word2vec_format.vec\")\n",
        "    #glove2word2vec(glove_file, word2vec_file)\n",
        "    print(\"Loading Glove vectors...\")\n",
        "    #word_vectors = KeyedVectors.load_word2vec_format(word2vec_file)\n",
        "\n",
        "    with open( default_path + \"glove/model_glove_300.pkl\", 'rb') as handle:\n",
        "        word_vectors = pickle.load(handle)\n",
        "        \n",
        "    word_vec_list = list()\n",
        "    for _, word in sorted(reversed_dict.items()):\n",
        "        try:\n",
        "            word_vec = word_vectors.word_vec(word)\n",
        "        except KeyError:\n",
        "            word_vec = np.zeros([embedding_size], dtype=np.float32)\n",
        "\n",
        "        word_vec_list.append(word_vec)\n",
        "\n",
        "    # Assign random vector to <s>, </s> token\n",
        "    word_vec_list[2] = np.random.normal(0, 1, embedding_size)\n",
        "    word_vec_list[3] = np.random.normal(0, 1, embedding_size)\n",
        "\n",
        "    return np.array(word_vec_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVRWvFUYSOmk",
        "colab_type": "code",
        "outputId": "53e0f037-20ad-4456-e201-18cef7deca48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "word_embedding = get_init_embedding(reversed_dict, 300)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading Glove vectors...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1IPEmUbSo-E",
        "colab_type": "code",
        "outputId": "178b6804-6877-4dd0-dcd3-b401e367348f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "word_dict, reversed_dict, article_max_len, summary_max_len = build_dict(\"train\", False)\n",
        "train_x, train_y = build_dataset(\"train\", word_dict, article_max_len, summary_max_len, False)\n",
        "word_embedding = get_init_embedding(reversed_dict, 300)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading Glove vectors...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPJ_hJjkUw0I",
        "colab_type": "code",
        "outputId": "431d406a-fb71-4e22-c78f-f384a8483a83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1292
        }
      },
      "source": [
        "print(word_embedding[1617])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 4.25179988e-01  5.27989984e-01 -8.34169984e-01  2.70269990e-01\n",
            "  2.17439998e-02 -4.47340012e-01 -1.71809995e+00  4.43639994e-01\n",
            "  1.84640005e-01 -7.52319992e-01  3.66809994e-01  2.55120009e-01\n",
            " -8.04229975e-01  1.20090000e-01 -1.39060006e-01  1.60530001e-01\n",
            " -8.88459980e-02  1.72680005e-01  1.03980005e+00  5.37400007e-01\n",
            "  4.39100005e-02 -2.97729999e-01  3.56389999e-01 -7.37649977e-01\n",
            " -7.68209994e-01 -3.06189992e-02  1.17409997e-01  3.00069988e-01\n",
            " -3.32549989e-01  1.71540007e-01  1.26040005e-03  4.13599998e-01\n",
            "  3.76080006e-01 -4.92949992e-01 -7.14800000e-01 -1.31259993e-01\n",
            "  9.99069989e-01 -1.15940003e-02  7.52009973e-02 -2.59910002e-02\n",
            "  9.86030046e-03  9.29249972e-02 -5.19939996e-02  9.12329972e-01\n",
            "  1.72179993e-02 -6.59460008e-01  6.23040013e-02  1.13490000e-01\n",
            "  8.95460024e-02  4.36629988e-02  3.79839987e-01  9.38510001e-02\n",
            " -6.13499999e-01  3.83269995e-01  2.20159993e-01  9.25650001e-02\n",
            "  1.28030002e-01  4.11910005e-02  2.62430012e-01  4.84310016e-02\n",
            " -2.52920002e-01 -7.40339994e-01  1.67699993e-01  2.97179997e-01\n",
            " -4.17340010e-01 -1.10440004e+00  3.96100014e-01 -3.35550010e-01\n",
            " -1.26690000e-01 -2.35070005e-01  2.83289999e-01  9.60240006e-01\n",
            " -2.12290004e-01  2.66180009e-01  1.94159999e-01 -3.93049985e-01\n",
            "  4.31389987e-01  2.93669999e-01  6.18380010e-01  8.71259987e-01\n",
            "  1.49550006e-01  7.95179978e-02 -1.99129999e-01  4.62760001e-01\n",
            " -2.64640003e-01 -5.53859994e-02  9.87920016e-02  3.65819991e-01\n",
            " -4.77270000e-02 -1.20660000e-01  9.55640018e-01  6.42700016e-01\n",
            " -4.88790005e-01 -4.96499985e-02  4.86409992e-01  4.08659987e-02\n",
            " -2.11919993e-01 -7.73370028e-01 -4.56750005e-01 -1.78849995e-01\n",
            "  2.45710000e-01 -4.64500010e-01 -4.95209992e-01  3.58759999e-01\n",
            "  6.93650007e-01 -1.38370007e-01  3.12559992e-01 -1.22010000e-02\n",
            " -5.02699971e-01 -2.71490008e-01 -6.45070001e-02 -8.38110030e-01\n",
            " -7.77719989e-02 -1.42399997e-01  2.50479996e-01  3.41159999e-01\n",
            "  1.50160000e-01 -5.69610000e-02 -1.52229995e-01  6.45519972e-01\n",
            "  2.38840003e-02  1.84349999e-01  1.59189999e-01 -3.91640007e-01\n",
            " -2.24010006e-01 -2.32350007e-01  5.39260030e-01 -5.66550016e-01\n",
            "  1.61300004e-01  3.23509991e-01 -5.34569984e-03 -6.81550026e-01\n",
            " -4.90139991e-01  6.97920024e-01  2.55320013e-01 -1.26829997e-01\n",
            " -3.98460001e-01 -3.09450001e-01  1.81930006e-01  4.39940006e-01\n",
            "  3.93570006e-01 -5.10209978e-01  1.39750004e-01  6.20410025e-01\n",
            "  6.04300022e-01 -4.22219992e-01  2.20819995e-01  5.80470026e-01\n",
            " -6.43090010e-01 -8.57420027e-01  1.30599999e+00 -3.08920001e-03\n",
            "  2.65850008e-01  3.23060006e-01  1.02629997e-01 -2.76560009e-01\n",
            "  6.37319982e-02  1.39469996e-01 -1.11019999e-01 -3.57930005e-01\n",
            "  3.39709997e-01  5.25039971e-01 -5.98269999e-02  1.75119996e-01\n",
            "  1.63550004e-01  6.18989989e-02  2.91869994e-02 -3.67159992e-02\n",
            " -4.02909994e-01 -9.96579975e-02 -4.75540012e-01  4.53669995e-01\n",
            " -1.71159998e-01 -1.83690004e-02 -2.89819986e-01 -3.36829990e-01\n",
            " -3.44650000e-01  7.44029999e-01  1.79649994e-01 -1.85920000e-01\n",
            "  3.92349988e-01  4.35240000e-01  4.21249986e-01  5.38649976e-01\n",
            " -2.11500004e-01  8.05319995e-02 -1.25109994e+00 -4.16559994e-01\n",
            "  1.22749999e-01  9.39100027e-01 -3.42950001e-02  2.04039998e-02\n",
            "  4.82270002e-01 -4.73960005e-02  1.54259995e-01 -8.38380009e-02\n",
            "  4.16150004e-01  1.21130003e-03  1.12949997e-01  7.28250027e-01\n",
            "  5.87830007e-01  4.02020007e-01  5.04040003e-01 -3.86469990e-01\n",
            "  1.37940004e-01 -2.55199999e-01  1.82589993e-01  3.36829990e-01\n",
            "  3.10330003e-01  5.09999990e-01  2.10529998e-01 -4.05470014e-01\n",
            "  1.78739995e-01  3.76430005e-01 -2.92750001e-01  4.97159988e-01\n",
            "  1.28309995e-01 -8.12259972e-01 -3.11479986e-01 -1.01070002e-01\n",
            "  5.10590017e-01  4.06949997e-01 -2.63579994e-01  3.46340001e-01\n",
            "  5.41639984e-01 -2.15259999e-01  5.73580027e-01 -5.83150029e-01\n",
            " -2.47170001e-01  4.83190000e-01 -8.70760024e-01  3.38369995e-01\n",
            "  2.21780002e-01  6.40460029e-02  3.04410011e-01 -4.86220002e-01\n",
            " -3.84110004e-01  1.61919996e-01  1.69149995e-01  1.57479998e-02\n",
            " -2.84889996e-01  1.00409999e-01 -7.33910024e-01 -1.58929992e-02\n",
            "  4.51409996e-01  6.86399996e-01 -8.83190036e-02  1.82799995e-01\n",
            " -1.93590000e-01 -4.79530007e-01  5.15999973e-01  1.32459998e-02\n",
            " -5.18469989e-01  5.15550017e-01  6.71140030e-02  6.09750003e-02\n",
            "  5.30359983e-01  4.42870008e-03 -7.34689981e-02 -3.53570014e-01\n",
            " -4.80859995e-01  6.35190010e-01  3.75000000e-01  3.97399992e-01\n",
            " -5.70949987e-02  6.29119992e-01  2.25779995e-01  6.50799990e-01\n",
            " -2.53789991e-01 -7.27989972e-01 -6.42949998e-01  1.73639998e-01\n",
            "  4.51519992e-03  1.00259997e-01 -2.48150006e-02  5.41660011e-01\n",
            " -8.05850029e-01 -2.29790002e-01  1.12130001e-01 -1.01219997e-01\n",
            " -1.30689994e-01  6.14619970e-01  2.13640004e-01  2.43129998e-01\n",
            " -4.65380013e-01  4.64960009e-01 -3.64569992e-01 -8.38110000e-02\n",
            " -2.49640003e-01  1.94470003e-01 -1.73280001e-01  4.78439987e-01\n",
            " -4.68230009e-01 -1.37899995e-01  2.60820001e-01  6.67779982e-01\n",
            " -6.25090003e-01 -1.70530006e-01 -3.65859985e-01  4.90099996e-01]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDhyjfQ0YXdb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.contrib import rnn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmGzSF_2zMSh",
        "colab_type": "code",
        "outputId": "eea2cb62-9c06-4a7b-c2f5-d226ac121fae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "X = tf.placeholder(tf.int32, [None, article_max_len])\n",
        "global_step = tf.Variable(0, trainable=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKLfJfEPUR93",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model(object):\n",
        "    def __init__(self, reversed_dict, article_max_len, summary_max_len, args, forward_only=False):\n",
        "        self.vocabulary_size = len(reversed_dict)\n",
        "        self.embedding_size = args.embedding_size\n",
        "        self.num_hidden = args.num_hidden\n",
        "        self.num_layers = args.num_layers\n",
        "        self.learning_rate = args.learning_rate\n",
        "        self.beam_width = args.beam_width\n",
        "        \n",
        "        if not forward_only:\n",
        "            self.keep_prob = args.keep_prob\n",
        "        else:\n",
        "            self.keep_prob = 1.0\n",
        "        self.cell = tf.nn.rnn_cell.BasicLSTMCell\n",
        "        with tf.variable_scope(\"decoder/projection\"):\n",
        "            self.projection_layer = tf.layers.Dense(self.vocabulary_size, use_bias=False)\n",
        "\n",
        "        self.batch_size = tf.placeholder(tf.int32, (), name=\"batch_size\")\n",
        "        self.X = tf.placeholder(tf.int32, [None, article_max_len])\n",
        "        self.X_len = tf.placeholder(tf.int32, [None])\n",
        "        self.decoder_input = tf.placeholder(tf.int32, [None, summary_max_len])\n",
        "        self.decoder_len = tf.placeholder(tf.int32, [None])\n",
        "        self.decoder_target = tf.placeholder(tf.int32, [None, summary_max_len])\n",
        "        self.global_step = tf.Variable(0, trainable=False)\n",
        "        with tf.name_scope(\"embedding\"):\n",
        "            if not forward_only and args.glove:\n",
        "                init_embeddings = tf.constant(get_init_embedding(reversed_dict, self.embedding_size), dtype=tf.float32)\n",
        "            else:\n",
        "                init_embeddings = tf.random_uniform([self.vocabulary_size, self.embedding_size], -1.0, 1.0)\n",
        "            self.embeddings = tf.get_variable(\"embeddings\", initializer=init_embeddings)\n",
        "            self.encoder_emb_inp = tf.transpose(tf.nn.embedding_lookup(self.embeddings, self.X), perm=[1, 0, 2])\n",
        "            self.decoder_emb_inp = tf.transpose(tf.nn.embedding_lookup(self.embeddings, self.decoder_input), perm=[1, 0, 2])\n",
        "\n",
        "        with tf.name_scope(\"encoder\"):\n",
        "            fw_cells = [self.cell(self.num_hidden) for _ in range(self.num_layers)]\n",
        "            bw_cells = [self.cell(self.num_hidden) for _ in range(self.num_layers)]\n",
        "            fw_cells = [rnn.DropoutWrapper(cell) for cell in fw_cells]\n",
        "            bw_cells = [rnn.DropoutWrapper(cell) for cell in bw_cells]\n",
        "\n",
        "            encoder_outputs, encoder_state_fw, encoder_state_bw = tf.contrib.rnn.stack_bidirectional_dynamic_rnn(\n",
        "                fw_cells, bw_cells, self.encoder_emb_inp,\n",
        "                sequence_length=self.X_len, time_major=True, dtype=tf.float32)\n",
        "            self.encoder_output = tf.concat(encoder_outputs, 2)\n",
        "            encoder_state_c = tf.concat((encoder_state_fw[0].c, encoder_state_bw[0].c), 1)\n",
        "            encoder_state_h = tf.concat((encoder_state_fw[0].h, encoder_state_bw[0].h), 1)\n",
        "            self.encoder_state = rnn.LSTMStateTuple(c=encoder_state_c, h=encoder_state_h)\n",
        "\n",
        "        with tf.name_scope(\"decoder\"), tf.variable_scope(\"decoder\") as decoder_scope:\n",
        "            decoder_cell = self.cell(self.num_hidden * 2)\n",
        "\n",
        "            if not forward_only:\n",
        "                attention_states = tf.transpose(self.encoder_output, [1, 0, 2])\n",
        "                attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(\n",
        "                    self.num_hidden * 2, attention_states, memory_sequence_length=self.X_len, normalize=True)\n",
        "                decoder_cell = tf.contrib.seq2seq.AttentionWrapper(decoder_cell, attention_mechanism,\n",
        "                                                                   attention_layer_size=self.num_hidden * 2)\n",
        "                initial_state = decoder_cell.zero_state(dtype=tf.float32, batch_size=self.batch_size)\n",
        "                initial_state = initial_state.clone(cell_state=self.encoder_state)\n",
        "                helper = tf.contrib.seq2seq.TrainingHelper(self.decoder_emb_inp, self.decoder_len, time_major=True)\n",
        "                decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell, helper, initial_state)\n",
        "                outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, output_time_major=True, scope=decoder_scope)\n",
        "                self.decoder_output = outputs.rnn_output\n",
        "                self.logits = tf.transpose(\n",
        "                    self.projection_layer(self.decoder_output), perm=[1, 0, 2])\n",
        "                self.logits_reshape = tf.concat(\n",
        "                    [self.logits, tf.zeros([self.batch_size, summary_max_len - tf.shape(self.logits)[1], self.vocabulary_size])], axis=1)\n",
        "            else:\n",
        "                tiled_encoder_output = tf.contrib.seq2seq.tile_batch(\n",
        "                    tf.transpose(self.encoder_output, perm=[1, 0, 2]), multiplier=self.beam_width)\n",
        "                tiled_encoder_final_state = tf.contrib.seq2seq.tile_batch(self.encoder_state, multiplier=self.beam_width)\n",
        "                tiled_seq_len = tf.contrib.seq2seq.tile_batch(self.X_len, multiplier=self.beam_width)\n",
        "                attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(\n",
        "                    self.num_hidden * 2, tiled_encoder_output, memory_sequence_length=tiled_seq_len, normalize=True)\n",
        "                decoder_cell = tf.contrib.seq2seq.AttentionWrapper(decoder_cell, attention_mechanism,\n",
        "                                                                   attention_layer_size=self.num_hidden * 2)\n",
        "                initial_state = decoder_cell.zero_state(dtype=tf.float32, batch_size=self.batch_size * self.beam_width)\n",
        "                initial_state = initial_state.clone(cell_state=tiled_encoder_final_state)\n",
        "                decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n",
        "                    cell=decoder_cell,\n",
        "                    embedding=self.embeddings,\n",
        "                    start_tokens=tf.fill([self.batch_size], tf.constant(2)),\n",
        "                    end_token=tf.constant(3),\n",
        "                    initial_state=initial_state,\n",
        "                    beam_width=self.beam_width,\n",
        "                    output_layer=self.projection_layer\n",
        "                )\n",
        "                outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
        "                    decoder, output_time_major=True, maximum_iterations=summary_max_len, scope=decoder_scope)\n",
        "                self.prediction = tf.transpose(outputs.predicted_ids, perm=[1, 2, 0])\n",
        "\n",
        "        with tf.name_scope(\"loss\"):\n",
        "            if not forward_only:\n",
        "                crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "                    logits=self.logits_reshape, labels=self.decoder_target)\n",
        "                weights = tf.sequence_mask(self.decoder_len, summary_max_len, dtype=tf.float32)\n",
        "                self.loss = tf.reduce_sum(crossent * weights / tf.to_float(self.batch_size))\n",
        "\n",
        "                params = tf.trainable_variables()\n",
        "                gradients = tf.gradients(self.loss, params)\n",
        "                clipped_gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
        "                optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
        "                self.update = optimizer.apply_gradients(zip(clipped_gradients, params), global_step=self.global_step)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXFIlYo4eRhG",
        "colab_type": "code",
        "outputId": "8f87bc6b-08d8-4dcf-c319-9b5730092ac8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1482
        }
      },
      "source": [
        "import time\n",
        "start = time.perf_counter()\n",
        "import tensorflow as tf\n",
        "import argparse\n",
        "import pickle\n",
        "import os\n",
        "#from model import Model\n",
        "#from utils import build_dict, build_dataset, batch_iter\n",
        "\n",
        "# Uncomment next 2 lines to suppress error and Tensorflow info verbosity. Or change logging levels\n",
        "# tf.logging.set_verbosity(tf.logging.FATAL)\n",
        "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n",
        "#def add_arguments(parser):\n",
        "#    parser.add_argument(\"--num_hidden\", type=int, default=150, help=\"Network size.\")\n",
        "#    parser.add_argument(\"--num_layers\", type=int, default=2, help=\"Network depth.\")\n",
        "#    parser.add_argument(\"--beam_width\", type=int, default=10, help=\"Beam width for beam search decoder.\")\n",
        "#    parser.add_argument(\"--glove\", action=\"store_true\", help=\"Use glove as initial word embedding.\")\n",
        "#    parser.add_argument(\"--embedding_size\", type=int, default=300, help=\"Word embedding size.\")\n",
        "#\n",
        "#    parser.add_argument(\"--learning_rate\", type=float, default=1e-3, help=\"Learning rate.\")\n",
        "#    parser.add_argument(\"--batch_size\", type=int, default=64, help=\"Batch size.\")\n",
        "#    parser.add_argument(\"--num_epochs\", type=int, default=10, help=\"Number of epochs.\")\n",
        "#    parser.add_argument(\"--keep_prob\", type=float, default=0.8, help=\"Dropout keep prob.\")\n",
        "#\n",
        "#    parser.add_argument(\"--toy\", action=\"store_true\", help=\"Use only 50K samples of data\")\n",
        "#\n",
        "#    parser.add_argument(\"--with_model\", action=\"store_true\", help=\"Continue from previously saved model\")\n",
        "\n",
        "class args:\n",
        "    pass\n",
        "  \n",
        "args.num_hidden=150\n",
        "args.num_layers=2\n",
        "args.beam_width=10\n",
        "args.glove=\"store_true\"\n",
        "args.embedding_size=300\n",
        "\n",
        "args.learning_rate=1e-3\n",
        "args.batch_size=64\n",
        "args.num_epochs=10\n",
        "args.keep_prob = 0.8\n",
        "\n",
        "args.toy=False #\"store_true\"\n",
        "\n",
        "args.with_model=\"store_true\"\n",
        "\n",
        "\n",
        "#parser = argparse.ArgumentParser()\n",
        "#add_arguments(parser)\n",
        "#args = parser.parse_args()\n",
        "#with open(\"args.pickle\", \"wb\") as f:\n",
        "#    pickle.dump(args, f)\n",
        "\n",
        "if not os.path.exists(default_path + \"saved_model\"):\n",
        "    os.mkdir(default_path + \"saved_model\")\n",
        "else:\n",
        "    #if args.with_model:\n",
        "        old_model_checkpoint_path = open(default_path + 'saved_model/checkpoint', 'r')\n",
        "        old_model_checkpoint_path = \"\".join([default_path + \"saved_model/\",old_model_checkpoint_path.read().splitlines()[0].split('\"')[1] ])\n",
        "\n",
        "\n",
        "print(\"Building dictionary...\")\n",
        "word_dict, reversed_dict, article_max_len, summary_max_len = build_dict(\"train\", args.toy)\n",
        "print(\"Loading training dataset...\")\n",
        "train_x, train_y = build_dataset(\"train\", word_dict, article_max_len, summary_max_len, args.toy)\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    model = Model(reversed_dict, article_max_len, summary_max_len, args)\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    saver = tf.train.Saver(tf.global_variables())\n",
        "    if 'old_model_checkpoint_path' in globals():\n",
        "        print(\"Continuing from previous trained model:\" , old_model_checkpoint_path , \"...\")\n",
        "        saver.restore(sess, old_model_checkpoint_path )\n",
        "\n",
        "    batches = batch_iter(train_x, train_y, args.batch_size, args.num_epochs)\n",
        "    num_batches_per_epoch = (len(train_x) - 1) // args.batch_size + 1\n",
        "\n",
        "    print(\"\\nIteration starts.\")\n",
        "    print(\"Number of batches per epoch :\", num_batches_per_epoch)\n",
        "    for batch_x, batch_y in batches:\n",
        "        batch_x_len = list(map(lambda x: len([y for y in x if y != 0]), batch_x))\n",
        "        batch_decoder_input = list(map(lambda x: [word_dict[\"<s>\"]] + list(x), batch_y))\n",
        "        batch_decoder_len = list(map(lambda x: len([y for y in x if y != 0]), batch_decoder_input))\n",
        "        batch_decoder_output = list(map(lambda x: list(x) + [word_dict[\"</s>\"]], batch_y))\n",
        "\n",
        "        batch_decoder_input = list(\n",
        "            map(lambda d: d + (summary_max_len - len(d)) * [word_dict[\"<padding>\"]], batch_decoder_input))\n",
        "        batch_decoder_output = list(\n",
        "            map(lambda d: d + (summary_max_len - len(d)) * [word_dict[\"<padding>\"]], batch_decoder_output))\n",
        "\n",
        "        train_feed_dict = {\n",
        "            model.batch_size: len(batch_x),\n",
        "            model.X: batch_x,\n",
        "            model.X_len: batch_x_len,\n",
        "            model.decoder_input: batch_decoder_input,\n",
        "            model.decoder_len: batch_decoder_len,\n",
        "            model.decoder_target: batch_decoder_output\n",
        "        }\n",
        "\n",
        "        _, step, loss = sess.run([model.update, model.global_step, model.loss], feed_dict=train_feed_dict)\n",
        "\n",
        "        if step % 1000 == 0:\n",
        "            print(\"step {0}: loss = {1}\".format(step, loss))\n",
        "\n",
        "        if step % num_batches_per_epoch == 0:\n",
        "            hours, rem = divmod(time.perf_counter() - start, 3600)\n",
        "            minutes, seconds = divmod(rem, 60)\n",
        "            saver.save(sess, default_path + \"saved_model/model.ckpt\", global_step=step)\n",
        "            print(\" Epoch {0}: Model is saved.\".format(step // num_batches_per_epoch),\n",
        "            \"Elapsed: {:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds) , \"\\n\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building dictionary...\n",
            "Loading training dataset...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading Glove vectors...\n",
            "WARNING:tensorflow:From <ipython-input-29-bca1bf609930>:35: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/rnn/python/ops/rnn.py:233: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From <ipython-input-29-bca1bf609930>:96: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "\n",
            "Iteration starts.\n",
            "Number of batches per epoch : 3125\n",
            "step 1000: loss = 48.88435745239258\n",
            "step 2000: loss = 36.74921417236328\n",
            "step 3000: loss = 35.5855827331543\n",
            " Epoch 1: Model is saved. Elapsed: 00:15:58.36 \n",
            "\n",
            "step 4000: loss = 33.543312072753906\n",
            "step 5000: loss = 36.146907806396484\n",
            "step 6000: loss = 26.449857711791992\n",
            " Epoch 2: Model is saved. Elapsed: 00:28:42.87 \n",
            "\n",
            "step 7000: loss = 30.42853546142578\n",
            "step 8000: loss = 22.401227951049805\n",
            "step 9000: loss = 21.89345359802246\n",
            " Epoch 3: Model is saved. Elapsed: 00:41:32.29 \n",
            "\n",
            "step 10000: loss = 20.575836181640625\n",
            "step 11000: loss = 17.46924591064453\n",
            "step 12000: loss = 14.658700942993164\n",
            " Epoch 4: Model is saved. Elapsed: 00:54:21.32 \n",
            "\n",
            "step 13000: loss = 19.396867752075195\n",
            "step 14000: loss = 12.515384674072266\n",
            "step 15000: loss = 17.88766098022461\n",
            " Epoch 5: Model is saved. Elapsed: 01:07:03.40 \n",
            "\n",
            "step 16000: loss = 17.06005096435547\n",
            "step 17000: loss = 16.666452407836914\n",
            "step 18000: loss = 15.164145469665527\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            " Epoch 6: Model is saved. Elapsed: 01:19:53.73 \n",
            "\n",
            "step 19000: loss = 15.926368713378906\n",
            "step 20000: loss = 13.397058486938477\n",
            "step 21000: loss = 10.665632247924805\n",
            " Epoch 7: Model is saved. Elapsed: 01:32:53.58 \n",
            "\n",
            "step 22000: loss = 9.421197891235352\n",
            "step 23000: loss = 10.845868110656738\n",
            "step 24000: loss = 8.716793060302734\n",
            "step 25000: loss = 14.262184143066406\n",
            " Epoch 8: Model is saved. Elapsed: 01:45:49.52 \n",
            "\n",
            "step 26000: loss = 15.93642520904541\n",
            "step 27000: loss = 9.916559219360352\n",
            "step 28000: loss = 12.064785957336426\n",
            " Epoch 9: Model is saved. Elapsed: 01:58:42.84 \n",
            "\n",
            "step 29000: loss = 11.197529792785645\n",
            "step 30000: loss = 15.267059326171875\n",
            "step 31000: loss = 8.1036958694458\n",
            " Epoch 10: Model is saved. Elapsed: 02:11:39.21 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "monjP4bsHZRt",
        "colab_type": "code",
        "outputId": "3e4a5641-9cdf-4132-abb3-7d6f19c665d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import pickle\n",
        "#from model import Model\n",
        "#from utils import build_dict, build_dataset, batch_iter\n",
        "\n",
        "\n",
        "#with open(\"args.pickle\", \"rb\") as f:\n",
        "#    args = pickle.load(f)\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "class args:\n",
        "    pass\n",
        "  \n",
        "args.num_hidden=150\n",
        "args.num_layers=2\n",
        "args.beam_width=10\n",
        "args.glove=\"store_true\"\n",
        "args.embedding_size=300\n",
        "\n",
        "args.learning_rate=1e-3\n",
        "args.batch_size=64\n",
        "args.num_epochs=10\n",
        "args.keep_prob = 0.8\n",
        "\n",
        "args.toy=True\n",
        "\n",
        "args.with_model=\"store_true\"\n",
        "\n",
        "\n",
        "\n",
        "print(\"Loading dictionary...\")\n",
        "word_dict, reversed_dict, article_max_len, summary_max_len = build_dict(\"valid\", args.toy)\n",
        "print(\"Loading validation dataset...\")\n",
        "valid_x = build_dataset(\"valid\", word_dict, article_max_len, summary_max_len, args.toy)\n",
        "valid_x_len = [len([y for y in x if y != 0]) for x in valid_x]\n",
        "print(\"Loading article and reference...\")\n",
        "article = get_text_list(valid_article_path, args.toy)\n",
        "reference = get_text_list(valid_title_path, args.toy)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    print(\"Loading saved model...\")\n",
        "    model = Model(reversed_dict, article_max_len, summary_max_len, args, forward_only=True)\n",
        "    saver = tf.train.Saver(tf.global_variables())\n",
        "    ckpt = tf.train.get_checkpoint_state(default_path + \"saved_model/\")\n",
        "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
        "\n",
        "    batches = batch_iter(valid_x, [0] * len(valid_x), args.batch_size, 1)\n",
        "\n",
        "    print(\"Writing summaries to 'result.txt'...\")\n",
        "    for batch_x, _ in batches:\n",
        "        batch_x_len = [len([y for y in x if y != 0]) for x in batch_x]\n",
        "\n",
        "        valid_feed_dict = {\n",
        "            model.batch_size: len(batch_x),\n",
        "            model.X: batch_x,\n",
        "            model.X_len: batch_x_len,\n",
        "        }\n",
        "\n",
        "        prediction = sess.run(model.prediction, feed_dict=valid_feed_dict)\n",
        "        prediction_output = [[reversed_dict[y] for y in x] for x in prediction[:, 0, :]]\n",
        "        summary_array = []\n",
        "        with open(default_path + \"result.txt\", \"a\") as f:\n",
        "            for line in prediction_output:\n",
        "                summary = list()\n",
        "                for word in line:\n",
        "                    if word == \"</s>\":\n",
        "                        break\n",
        "                    if word not in summary:\n",
        "                        summary.append(word)\n",
        "                summary_array.append(\" \".join(summary))\n",
        "                #print(\" \".join(summary), file=f)\n",
        "\n",
        "    print('Summaries have been generated')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading dictionary...\n",
            "Loading validation dataset...\n",
            "Hi Prabha, here you are\n",
            "[[12711, 54, 638, 8457, 17416, 3703, 25, 5, 4, 16, 1368, 4423, 925, 11, 23, 6, 103, 49, 3972, 16, 4423, 65, 15, 5, 1774, 7, 4490, 19, 5, 4, 9459, 1703, 4], [16, 523, 186, 3051, 85, 23, 19, 2372, 32, 116, 22846, 712, 15, 16630, 670, 1341, 4], [207, 1935, 686, 4, 14, 23, 40, 16, 422, 348, 4, 33, 8, 829, 13, 160, 33, 8, 4, 17, 5, 480, 590, 1355, 25, 1310, 373, 422, 4], [500, 3525, 75, 1766, 11, 23, 2325, 11, 5, 379, 3075, 8, 3665, 15, 10, 376, 1379, 13, 22, 198, 243, 147, 7, 2916, 5, 1013, 258, 12, 152, 1774, 7, 1107, 5, 397, 12, 3540, 54, 151, 4], [16, 58, 3519, 7278, 205, 13, 16, 235, 9, 76, 4293, 1886, 49, 6406, 5, 74, 105, 832, 114, 171, 19, 5, 4459, 9, 1624, 12, 2710, 13674, 3523, 21826, 6, 5, 407, 253, 14, 23, 4], [3055, 1, 6, 10, 97, 19759, 19, 5, 563, 9, 10, 24649, 212, 762, 8, 452, 6, 11, 23, 2833, 1042, 8, 10, 16, 107, 8, 3894, 7, 11320, 8954, 8, 10, 2129, 5998, 1466, 217, 4], [1627, 12, 7829, 35, 13, 219, 1827, 67, 486, 10, 2494, 8, 10, 2170, 612, 38, 5, 10944, 2447, 15, 61, 5578, 5336, 46, 6, 10, 2039, 81, 14, 23, 4], [10, 4, 2347, 241, 26, 455, 391, 8, 1048, 673, 15, 28, 38, 1000, 104, 983, 203, 355, 7, 987, 16002, 6, 170, 13, 455, 65, 14, 4], [1617, 8639, 1236, 37, 2667, 17, 1433, 111, 27111, 11, 23, 159, 68, 2243, 5, 1247, 1709, 9, 5, 4, 687, 1829, 25, 11371, 474, 4], [2101, 55, 1258, 10, 36, 12793, 7, 11630, 15607, 51, 1757, 12, 3560, 6, 59, 17, 542, 330, 22175, 165, 7, 41504, 37, 757, 18, 10, 13144, 9114, 8, 51, 1757, 12, 4978, 6, 59, 5, 3657, 318, 14, 23, 4], [491, 6497, 40, 1751, 23, 7, 1363, 10214, 810, 7, 845, 6, 189, 5, 3013, 9, 10, 1868, 403, 3458, 8, 5, 4088, 2259, 4], [16, 971, 422, 49, 862, 64, 1859, 8, 4, 6, 10, 346, 933, 19, 2969, 1765, 225, 14, 23, 4], [5, 74, 105, 11, 23, 1060, 611, 12, 172, 437, 9, 10, 155, 1002, 609, 15, 12677, 7315, 2641, 103, 14, 43, 1144, 1777, 38, 5, 1395, 9, 1657, 8, 5, 54, 12, 369, 7333, 604, 4], [45, 93, 391, 11, 23, 1335, 4358, 934, 4, 6, 4, 1025, 47, 4, 6, 4, 347, 48, 25, 5, 3962, 9, 92, 97, 1214, 162, 6545, 4], [112, 91, 42, 1573, 909, 34, 1335, 7, 532, 172, 15, 10, 673, 1115, 7727, 24, 7806, 31814, 23, 2325, 6, 37, 302, 14, 4], [422, 9, 6092, 29, 30, 31, 1927, 8, 5, 74, 105, 613, 87, 137, 3983, 38, 5, 119, 339, 140, 6, 10, 399, 1857, 393, 14, 23, 4], [5, 2009, 2045, 9384, 4274, 66, 281, 35525, 50881, 1, 25, 5, 36, 495, 12672, 11, 23, 8, 10, 1, 10730, 4], [3336, 2101, 26534, 8919, 30597, 6, 94, 34, 3912, 7, 532, 114, 100, 24, 2075, 22, 14107, 166, 6, 39, 1335, 534, 10, 2483, 647, 17, 5, 6193, 10066, 38258, 4], [5, 407, 253, 12897, 722, 23, 6449, 32, 16, 307, 8, 60, 1220, 118, 22186, 7, 342, 11, 917, 6, 24, 10, 551, 69, 423, 173, 40509, 6, 364, 45, 529, 4], [5, 16, 707, 558, 7, 5547, 79, 607, 19, 1097, 188, 190, 12, 196, 785, 1137, 39, 1336, 6, 5, 76, 545, 14, 23, 185, 89, 4481, 1009, 5, 1334, 38, 5, 196, 942, 4], [19, 228, 45, 75, 67, 4419, 968, 15, 5, 2779, 962, 1661, 8, 381, 285, 6, 440, 42, 4488, 40557, 382, 10, 154, 528, 23, 4], [177, 12, 70, 558, 11, 23, 375, 2269, 111, 316, 6, 364, 2150, 150, 2445, 6, 15, 5, 180, 170, 47, 3578, 48, 1615, 477, 8, 665, 12, 818, 1609, 322, 7, 1741, 104, 11, 5, 751, 4], [157, 86, 42, 1658, 6242, 193, 23, 15, 5, 839, 9, 5, 70, 104, 286, 13, 914, 316, 15, 948, 6, 128, 6, 596, 13, 22, 180, 84, 7, 657, 5, 286, 4], [45, 615, 335, 1942, 52, 69, 23, 8, 685, 17, 130, 445, 299, 5, 4139, 2896, 11, 5, 335, 17, 276, 6, 10, 920, 906, 14, 4], [4318, 8542, 13, 37, 1567, 147, 1893, 184, 9, 91, 42, 871, 2043, 12, 9714, 23, 6, 633, 251, 491, 12, 499, 233, 11, 696, 4, 6, 412, 7, 5, 592, 705, 4], [627, 93, 8430, 8727, 34, 336, 23, 17, 40, 152, 16, 2880, 8, 45, 240, 24, 6892, 10, 46, 510, 82, 40, 7286, 3148, 8, 5, 834, 76, 9, 8126, 4], [5, 9583, 4921, 1485, 4391, 1, 18, 35, 13, 207, 954, 11, 23, 6, 16649, 1916, 9, 10, 6188, 24, 359, 1483, 7, 410, 5, 169, 471, 326, 5830, 4], [5072, 4466, 10, 9920, 9092, 1106, 15, 10, 22569, 1214, 162, 41, 23, 17, 10, 4, 73, 4, 262, 2273, 7, 3646, 25, 61, 1848, 2677, 5810, 9, 2546, 17477, 13, 6106, 4552, 4], [5, 126, 348, 7, 10, 7171, 888, 44, 5, 436, 23, 11, 1916, 32, 16, 385, 621, 49, 1041, 802, 1913, 53, 6, 16535, 5, 3250, 9, 40, 480, 640, 1871, 4], [45, 145, 16, 2063, 659, 3196, 61, 4, 2524, 8389, 23, 6, 17, 2163, 3500, 51, 42374, 2788, 6, 59, 330, 22175, 12, 51, 1235, 929, 13, 1235, 59, 7847, 13, 51, 573, 59, 594, 172, 1375, 8, 5, 4363, 643, 4], [1019, 15838, 1, 1, 15952, 10, 4340, 6, 4, 2292, 709, 17, 5, 10482, 2449, 11, 23, 6, 10, 100, 24, 68, 34, 89, 18, 108, 7, 10, 217, 17, 10606, 4], [16, 35, 330, 394, 4, 205, 14, 254, 23, 32, 68, 13, 5, 58, 3519, 4878, 51, 5, 497, 9, 5, 112, 75, 175, 91, 42, 1573, 909, 12, 440, 59, 13, 52, 11003, 15, 37, 717, 4], [5, 91, 42, 9, 4829, 5085, 6, 29, 30, 31, 17007, 1, 6, 39, 1469, 8, 37, 1269, 7, 35, 14445, 12952, 6, 5, 35, 12, 302, 127, 23, 4], [10, 145, 2101, 1012, 9734, 55, 911, 10, 107, 741, 7, 802, 18477, 13, 1100, 14209, 372, 8712, 25, 1076, 13111, 340, 13, 2123, 37, 662, 6, 107, 2239, 492, 23, 4], [5, 70, 104, 286, 49, 409, 10, 3209, 754, 11, 5, 1287, 8, 214, 12, 1344, 5617, 322, 254, 114, 171, 6, 29, 30, 31, 12, 70, 558, 32271, 29, 30, 31, 14, 23, 4], [212, 186, 8, 138, 74, 8, 3562, 15, 1573, 909, 11, 21, 18, 5, 91, 42, 5312, 694, 1648, 24, 2075, 10, 599, 6080, 4], [112, 91, 42, 1573, 909, 34, 5269, 22, 694, 727, 21, 24, 2075, 10, 599, 6080, 4], [329, 1833, 582, 203, 64, 11088, 4121, 25, 177, 13, 706, 6324, 3689, 7, 937, 215, 46, 2212, 1654, 21, 4], [54, 514, 45, 5895, 11620, 55, 8, 2228, 15, 114, 171, 12, 157, 198, 24, 5550, 11, 21, 25, 119, 100, 12, 16245, 1850, 111, 1069, 4], [8713, 6551, 1658, 41963, 34, 22161, 26, 36, 3867, 11, 23, 6, 102, 129, 24, 5, 13038, 1060, 187, 908, 1504, 31669, 13118, 4], [506, 42, 5021, 4700, 49, 6007, 22, 694, 223, 9, 5, 112, 683, 11, 21, 24, 91, 42, 1573, 909, 1312, 10, 599, 6080, 6, 376, 519, 89, 4], [2101, 2978, 5512, 35061, 39, 3782, 2243, 92, 140, 9, 13213, 2140, 15, 2847, 215, 5, 3434, 9, 1162, 6, 10, 789, 979, 11, 23, 4], [66, 190, 12, 912, 952, 5634, 5, 3057, 4, 4451, 919, 15, 5, 58, 239, 8, 423, 240, 8, 829, 6, 6006, 5, 84, 12, 2886, 161, 717, 6, 81, 465, 492, 21, 4], [2101, 542, 9503, 30537, 13, 3155, 24983, 11612, 67, 8886, 552, 7, 12656, 61, 2705, 13, 67, 1029, 7, 1086, 184, 17, 10, 4888, 6, 107, 2239, 492, 23, 4], [121, 122, 331, 62, 360, 113, 21, 19, 4, 73, 4, 4, 16, 71, 22, 1163, 6, 1212, 7, 23, 12, 108, 9, 4, 73, 4, 4, 71, 22, 1163, 4], [464, 106, 62, 304, 4, 33, 8, 466, 120, 21, 7, 336, 5, 1330, 919, 8, 87, 137, 174, 140, 18, 456, 324, 11, 197, 194, 12376, 2955, 1779, 1063, 41, 6, 109, 14, 4], [10, 145, 56, 949, 2495, 21, 7, 11130, 13863, 16826, 7, 353, 7, 177, 13, 444, 576, 185, 1039, 9, 29, 30, 31, 4], [5, 28935, 14523, 1312, 26, 112, 91, 42, 1573, 909, 32132, 159, 10, 11151, 15439, 8, 5, 4245, 13867, 1580, 43, 55, 698, 5, 5401, 9, 5, 213, 13976, 9, 6080, 4], [121, 122, 106, 62, 360, 4, 33, 113, 21, 11, 10351, 385, 8, 6797, 24, 23, 12, 1030, 324, 11, 16145, 385, 323, 1458, 6, 109, 14, 4], [120, 153, 66, 190, 13, 40, 681, 3816, 188, 190, 4717, 131, 72, 6, 7835, 87, 137, 102, 148, 71, 15, 5, 58, 239, 6, 81, 465, 492, 4]]\n",
            "Loading article and reference...\n",
            "Loading saved model...\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/seq2seq/python/ops/beam_search_decoder.py:733: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "INFO:tensorflow:Restoring parameters from drive/Colab Notebooks/saved_model/model.ckpt-31250\n",
            "Writing summaries to 'result.txt'...\n",
            "Summaries have been generated\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1dc7Mnal8Ts",
        "colab_type": "code",
        "outputId": "4de22799-6434-43a1-99c6-c1e51917ba5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "source": [
        "!pip3 install sumeval\n",
        "!python3 -m spacy download en"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sumeval\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d3/d0/23c9a37253044d478efede0770aefaea52a5b75911da56c3ac0aca894d2a/sumeval-0.2.0.tar.gz (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 2.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: plac>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from sumeval) (0.9.6)\n",
            "Collecting sacrebleu>=1.3.2 (from sumeval)\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/e4/0c186661604e70f4be57d44510115e63818189900cb0262bc5cd541bab3e/sacrebleu-1.3.5.tar.gz\n",
            "Requirement already satisfied: typing in /usr/local/lib/python3.6/dist-packages (from sacrebleu>=1.3.2->sumeval) (3.6.6)\n",
            "Building wheels for collected packages: sumeval, sacrebleu\n",
            "  Building wheel for sumeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/9b/50/fda7087af57d2efde31d4a85cecda98e683ed58eee07fbff2b\n",
            "  Building wheel for sacrebleu (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/f5/21/da/3e3dea18eb53af3b56d99b4b6ae50c91c89c8a7f422d676ee1\n",
            "Successfully built sumeval sacrebleu\n",
            "Installing collected packages: sacrebleu, sumeval\n",
            "Successfully installed sacrebleu-1.3.5 sumeval-0.2.0\n",
            "Requirement already satisfied: en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0 in /usr/local/lib/python3.6/dist-packages (2.0.0)\n",
            "\n",
            "\u001b[93m    Linking successful\u001b[0m\n",
            "    /usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "    /usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "\n",
            "    You can now load the model via spacy.load('en')\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fIYK7EQmX3J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#https://github.com/chakki-works/sumeval\n",
        "#https://github.com/Tian312/awesome-text-summarization\n",
        "\n",
        "from sumeval.metrics.rouge import RougeCalculator\n",
        "from sumeval.metrics.bleu import BLEUCalculator\n",
        "\n",
        "def eval_rouges(refrence_summary,model_summary):\n",
        "    #refrence_summary = \"tokyo shares close up #.## percent\"\n",
        "    #model_summary = \"tokyo stocks close up # percent to fresh record high\"\n",
        "\n",
        "    rouge = RougeCalculator(stopwords=True, lang=\"en\")\n",
        "\n",
        "    rouge_1 = rouge.rouge_n(\n",
        "                summary=model_summary,\n",
        "                references=refrence_summary,\n",
        "                n=1)\n",
        "\n",
        "    rouge_2 = rouge.rouge_n(\n",
        "                summary=model_summary,\n",
        "                references=[refrence_summary],\n",
        "                n=2)\n",
        "    \n",
        "    rouge_l = rouge.rouge_l(\n",
        "                summary=model_summary,\n",
        "                references=[refrence_summary])\n",
        "    \n",
        "    # You need spaCy to calculate ROUGE-BE\n",
        "    \n",
        "    rouge_be = rouge.rouge_be(\n",
        "                summary=model_summary,\n",
        "                references=[refrence_summary])\n",
        "\n",
        "    bleu = BLEUCalculator()\n",
        "    bleu_score = bleu.bleu( summary=model_summary,\n",
        "                        references=[refrence_summary])\n",
        "\n",
        "    #print(\"ROUGE-1: {}, ROUGE-2: {}, ROUGE-L: {}, ROUGE-BE: {}\".format(\n",
        "    #    rouge_1, rouge_2, rouge_l, rouge_be\n",
        "    #).replace(\", \", \"\\n\"))\n",
        "    \n",
        "    return rouge_1, rouge_2,rouge_l,rouge_be,bleu_score\n",
        "  \n",
        "#rouge_1, rouge_2,rouge_l,rouge_be = eval_rouges( \"tokyo shares close up #.## percent\",\n",
        "#                                                \"tokyo stocks close up # percent to fresh record high\")\n",
        "#\n",
        "#print(\"ROUGE-1: {}, ROUGE-2: {}, ROUGE-L: {}, ROUGE-BE: {}\".format(\n",
        "#        rouge_1, rouge_2, rouge_l, rouge_be\n",
        "#    ).replace(\", \", \"\\n\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AS3FQbwMmh7s",
        "colab_type": "code",
        "outputId": "1809177b-ce45-4bc3-a613-ed4219c5ee1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8058
        }
      },
      "source": [
        "#https://pymotw.com/2/xml/etree/ElementTree/create.html\n",
        "\n",
        "bleu_arr = []\n",
        "rouge_1_arr  = []\n",
        "rouge_2_arr  = []\n",
        "rouge_L_arr  = []\n",
        "rouge_be_arr = []\n",
        "\n",
        "from xml.etree import ElementTree\n",
        "from xml.dom import minidom\n",
        "from functools import reduce\n",
        "\n",
        "def prettify(elem):\n",
        "    \"\"\"Return a pretty-printed XML string for the Element.\n",
        "    \"\"\"\n",
        "    rough_string = ElementTree.tostring(elem, 'utf-8')\n",
        "    reparsed = minidom.parseString(rough_string)\n",
        "    return reparsed.toprettyxml(indent=\"  \")\n",
        "  \n",
        "from xml.etree.ElementTree import Element, SubElement, Comment\n",
        "\n",
        "top = Element('ZakSum')\n",
        "\n",
        "comment = Comment('Generated by Amr Zaki')\n",
        "top.append(comment)\n",
        "\n",
        "i=0\n",
        "for summ in summary_array:\n",
        "  example = SubElement(top, 'example')\n",
        "  article_element   = SubElement(example, 'article')\n",
        "  article_element.text = article[i]\n",
        "  \n",
        "  reference_element = SubElement(example, 'reference')\n",
        "  reference_element.text = reference[i]\n",
        "  \n",
        "  summary_element   = SubElement(example, 'summary')\n",
        "  summary_element.text = summ\n",
        "\n",
        "  rouge_1, rouge_2,rouge_L,rouge_be,bleu_score = eval_rouges(reference[i],summ )\n",
        "  print(\"Rouge 1 : \",rouge_1)\n",
        "  print(\"Rouge 2 : \",rouge_2)\n",
        "  print(\"Rouge L : \",rouge_L)\n",
        "  \n",
        "  eval_element = SubElement(example, 'eval')\n",
        "  bleu_score_element = SubElement(eval_element,'BLEU', {'score':str(bleu_score)})\n",
        "  ROUGE_1_element  = SubElement(eval_element, 'ROUGE_1' , {'score':str(rouge_1)})\n",
        "  ROUGE_2_element  = SubElement(eval_element, 'ROUGE_2' , {'score':str(rouge_2)})\n",
        "  ROUGE_L_element  = SubElement(eval_element, 'ROUGE_l' , {'score':str(rouge_L)})\n",
        "  ROUGE_be_element  = SubElement(eval_element,'ROUGE_be', {'score':str(rouge_be)})\n",
        "  \n",
        "  bleu_arr.append(bleu_score) \n",
        "  rouge_1_arr.append(rouge_1) \n",
        "  rouge_2_arr.append(rouge_2) \n",
        "  rouge_L_arr.append(rouge_L) \n",
        "  rouge_be_arr.append(rouge_be) \n",
        "\n",
        "  i+=1\n",
        "\n",
        "top.set('bleu', str(reduce(lambda x, y: x + y,  bleu_arr) / len(bleu_arr)))\n",
        "top.set('rouge_1', str(reduce(lambda x, y: x + y,  rouge_1_arr) / len(rouge_1_arr)))\n",
        "top.set('rouge_2', str(reduce(lambda x, y: x + y,  rouge_2_arr) / len(rouge_2_arr)))\n",
        "top.set('rouge_L', str(reduce(lambda x, y: x + y,  rouge_L_arr) / len(rouge_L_arr)))\n",
        "top.set('rouge_be', str(reduce(lambda x, y: x + y, rouge_be_arr) / len(rouge_be_arr)))\n",
        "\n",
        "with open(default_path + \"result_valid_29_10_2018_5_28pm.xml\", \"w+\") as f:\n",
        "  print(prettify(top), file=f)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b.olympic=(amod)=>champion\n",
            "a.injury=(compound)=>leaves\n",
            "<BasicElement: injury-[compound]->leave>\n",
            "a.kwan=(dobj)=>leaves\n",
            "<BasicElement: kwan-[dobj]->leave>\n",
            "b.olympic=(amod)=>hopes\n",
            "Rouge 1 :  0.2222222222222222\n",
            "Rouge 2 :  0\n",
            "Rouge L :  0.2222222222222222\n",
            "a.leaders=(nsubj)=>lash\n",
            "<BasicElement: leaders-[nsubj]->lash>\n",
            "b.proposed=(amod)=>immigrants\n",
            "b.illegal=(amod)=>immigrants\n",
            "b.tough=(amod)=>law\n",
            "Rouge 1 :  0.1818181818181818\n",
            "Rouge 2 :  0\n",
            "Rouge L :  0.1818181818181818\n",
            "a.sales=(nsubj)=>fall\n",
            "<BasicElement: sales-[nsubj]->fall>\n",
            "a.percent=(appos)=>fall\n",
            "<BasicElement: percent-[appos]->fall>\n",
            "b.second=(amod)=>quarter\n",
            "a.sales=(nsubj)=>fall\n",
            "<BasicElement: sales-[nsubj]->fall>\n",
            "a.percent=(npadvmod)=>fall\n",
            "<BasicElement: percent-[npadvmod]->fall>\n",
            "Rouge 1 :  0.8000000000000002\n",
            "Rouge 2 :  0.5\n",
            "Rouge L :  0.8000000000000002\n",
            "a.thousands=(nsubj)=>prepare\n",
            "<BasicElement: thousands-[nsubj]->prepare>\n",
            "a.cup=(dobj)=>prepare\n",
            "<BasicElement: cup-[dobj]->prepare>\n",
            "a.thousands=(nsubj)=>celebrate\n",
            "<BasicElement: thousands-[nsubj]->celebrate>\n",
            "Rouge 1 :  0.5454545454545454\n",
            "Rouge 2 :  0.22222222222222224\n",
            "Rouge L :  0.5454545454545454\n",
            "a.bush=(nsubj)=>attend\n",
            "<BasicElement: bush-[nsubj]->attend>\n",
            "a.inauguration=(dobj)=>attend\n",
            "<BasicElement: inauguration-[dobj]->attend>\n",
            "b.laura=(amod)=>bush\n",
            "b.attend=(advcl)=>bush\n",
            "a.inauguration=(dobj)=>attend\n",
            "<BasicElement: inauguration-[dobj]->attend>\n",
            "Rouge 1 :  0.6666666666666666\n",
            "Rouge 2 :  0.2\n",
            "Rouge L :  0.6666666666666666\n",
            "a.scandal=(nsubj)=>plead\n",
            "<BasicElement: scandal-[nsubj]->plead>\n",
            "b.top=(amod)=>lobbyist\n",
            "b.republican=(amod)=>lobbyist\n",
            "a.lobbyist=(nsubj)=>pleads\n",
            "<BasicElement: lobbyist-[nsubj]->plead>\n",
            "Rouge 1 :  0.30769230769230765\n",
            "Rouge 2 :  0.1818181818181818\n",
            "Rouge L :  0.30769230769230765\n",
            "b.somali=(amod)=>leaders\n",
            "a.leaders=(nsubj)=>agree\n",
            "<BasicElement: leaders-[nsubj]->agree>\n",
            "b.somalia=(compound)=>rivals\n",
            "Rouge 1 :  0.5454545454545454\n",
            "Rouge 2 :  0\n",
            "Rouge L :  0.3636363636363636\n",
            "b.planned=(acl)=>strike\n",
            "b.portuguese=(amod)=>workers\n",
            "a.strike=(nsubj)=>ground\n",
            "<BasicElement: strike-[nsubj]->ground>\n",
            "a.flights=(dobj)=>ground\n",
            "<BasicElement: flights-[dobj]->ground>\n",
            "Rouge 1 :  0.36363636363636365\n",
            "Rouge 2 :  0\n",
            "Rouge L :  0.36363636363636365\n",
            "b.renews=(compound)=>partnership\n",
            "Rouge 1 :  0.28571428571428575\n",
            "Rouge 2 :  0\n",
            "Rouge L :  0.28571428571428575\n",
            "b.new=(amod)=>site\n",
            "a.site=(nsubj)=>s\n",
            "<BasicElement: site-[nsubj]->s>\n",
            "b.be=(relcl)=>site\n",
            "a.man=(attr)=>be\n",
            "<BasicElement: man-[attr]->be>\n",
            "Rouge 1 :  0.25\n",
            "Rouge 2 :  0\n",
            "Rouge L :  0.25\n",
            "a.canada=(nsubj)=>advises\n",
            "<BasicElement: canada-[nsubj]->advise>\n",
            "a.nationals=(dobj)=>advises\n",
            "<BasicElement: nationals-[dobj]->advise>\n",
            "b.essential=(amod)=>travel\n",
            "a.travel=(dobj)=>make\n",
            "<BasicElement: travel-[dobj]->make>\n",
            "a.travel=(dobj)=>avoiding\n",
            "<BasicElement: travel-[dobj]->avoid>\n",
            "Rouge 1 :  0.5\n",
            "Rouge 2 :  0.2\n",
            "Rouge L :  0.5\n",
            "a.sales=(nsubj)=>be\n",
            "<BasicElement: sales-[nsubj]->be>\n",
            "a.executive=(nsubj)=>sees\n",
            "<BasicElement: executive-[nsubj]->see>\n",
            "b.weaker=(amod)=>sales\n",
            "a.sales=(dobj)=>sees\n",
            "<BasicElement: sales-[dobj]->see>\n",
            "Rouge 1 :  0.6666666666666666\n",
            "Rouge 2 :  0.28571428571428575\n",
            "Rouge L :  0.4444444444444444\n",
            "a.release=(dobj)=>welcomes\n",
            "<BasicElement: release-[dobj]->welcome>\n",
            "b.jailed=(amod)=>journalist\n",
            "b.chinese=(amod)=>journalist\n",
            "b.chinese=(amod)=>journalist\n",
            "a.journalist=(dobj)=>welcomes\n",
            "<BasicElement: journalist-[dobj]->welcome>\n",
            "a.concerns=(dobj)=>welcomes\n",
            "<BasicElement: concerns-[dobj]->welcome>\n",
            "Rouge 1 :  0.6153846153846153\n",
            "Rouge 2 :  0.1818181818181818\n",
            "Rouge L :  0.4615384615384615\n",
            "b.betting=(compound)=>target\n",
            "a.bankers=(nsubj)=>admit\n",
            "<BasicElement: bankers-[nsubj]->admit>\n",
            "a.theft=(dobj)=>admit\n",
            "<BasicElement: theft-[dobj]->admit>\n",
            "Rouge 1 :  0\n",
            "Rouge 2 :  0\n",
            "Rouge L :  0\n",
            "a.sharon=(nsubj)=>admitted\n",
            "<BasicElement: sharon-[nsubj]->admit>\n",
            "b.injured=(amod)=>man\n",
            "b.unwell=(amod)=>sharon\n",
            "a.sharon=(nsubj)=>admitted\n",
            "<BasicElement: sharon-[nsubj]->admit>\n",
            "Rouge 1 :  0.6\n",
            "Rouge 2 :  0.5\n",
            "Rouge L :  0.6\n",
            "a.sales=(dobj)=>hybrid\n",
            "<BasicElement: sales-[dobj]->hybrid>\n",
            "b.hybrid=(amod)=>vehicle\n",
            "a.sales=(nsubj)=>expect\n",
            "<BasicElement: sales-[nsubj]->expect>\n",
            "Rouge 1 :  0.4444444444444444\n",
            "Rouge 2 :  0\n",
            "Rouge L :  0.4444444444444444\n",
            "a.dodgers=(nsubj)=>acquire\n",
            "<BasicElement: dodgers-[nsubj]->acquire>\n",
            "b.south=(amod)=>korea\n",
            "a.korea=(compound)=>s\n",
            "<BasicElement: korea-[compound]->s>\n",
            "a.unk=(conj)=>acquire\n",
            "<BasicElement: unk-[conj]->acquire>\n",
            "b.headed=(acl)=>seo\n",
            "Rouge 1 :  0.4000000000000001\n",
            "Rouge 2 :  0\n",
            "Rouge L :  0.20000000000000004\n",
            "b.teenage=(amod)=>star\n",
            "a.star=(nmod)=>admits\n",
            "<BasicElement: star-[nmod]->admit>\n",
            "a.lohan=(amod)=>admits\n",
            "<BasicElement: lohan-[amod]->admit>\n",
            "a.lindsay=(nsubj)=>admits\n",
            "<BasicElement: lindsay-[nsubj]->admit>\n",
            "a.lohan=(amod)=>admits\n",
            "<BasicElement: lohan-[amod]->admit>\n",
            "a.battle=(dobj)=>admits\n",
            "<BasicElement: battle-[dobj]->admit>\n",
            "Rouge 1 :  0.30769230769230765\n",
            "Rouge 2 :  0.1818181818181818\n",
            "Rouge L :  0.30769230769230765\n",
            "b.white=(amod)=>house\n",
            "a.house=(nsubj)=>rejects\n",
            "<BasicElement: house-[nsubj]->reject>\n",
            "a.iraq=(dobj)=>rejects\n",
            "<BasicElement: iraq-[dobj]->reject>\n",
            "a.combat=(dobj)=>rejects\n",
            "<BasicElement: combat-[dobj]->reject>\n",
            "a.soldiers=(nsubj)=>act\n",
            "<BasicElement: soldiers-[nsubj]->act>\n",
            "a.civilians=(dobj)=>protect\n",
            "<BasicElement: civilians-[dobj]->protect>\n",
            "Rouge 1 :  0\n",
            "Rouge 2 :  0\n",
            "Rouge L :  0\n",
            "b.special=(amod)=>envoy\n",
            "b.korean=(amod)=>talks\n",
            "b.nuclear=(amod)=>talks\n",
            "a.talks=(nsubj)=>quits\n",
            "<BasicElement: talks-[nsubj]->quit>\n",
            "Rouge 1 :  0.4\n",
            "Rouge 2 :  0\n",
            "Rouge L :  0.4\n",
            "b.positive=(amod)=>tests\n",
            "b.positive=(amod)=>test\n",
            "Rouge 1 :  0.8333333333333334\n",
            "Rouge 2 :  0.8000000000000002\n",
            "Rouge L :  0.8333333333333334\n",
            "a.britain=(nsubj)=>urges\n",
            "<BasicElement: britain-[nsubj]->urge>\n",
            "a.inclusion=(dobj)=>urges\n",
            "<BasicElement: inclusion-[dobj]->urge>\n",
            "a.britain=(nsubj)=>urges\n",
            "<BasicElement: britain-[nsubj]->urge>\n",
            "b.stronger=(amod)=>support\n",
            "b.international=(amod)=>support\n",
            "a.support=(dobj)=>urges\n",
            "<BasicElement: support-[dobj]->urge>\n",
            "Rouge 1 :  0.4615384615384615\n",
            "Rouge 2 :  0.1818181818181818\n",
            "Rouge L :  0.4615384615384615\n",
            "b.australian=(amod)=>fm\n",
            "a.fm=(nsubj)=>calls\n",
            "<BasicElement: fm-[nsubj]->call>\n",
            "a.australia=(amod)=>backs\n",
            "<BasicElement: australia-[amod]->back>\n",
            "Rouge 1 :  0.3636363636363636\n",
            "Rouge 2 :  0.22222222222222224\n",
            "Rouge L :  0.3636363636363636\n",
            "b.egyptian=(amod)=>guards\n",
            "b.killed=(acl)=>guards\n",
            "b.egyptian=(amod)=>guards\n",
            "b.killed=(acl)=>guards\n",
            "Rouge 1 :  0.8000000000000002\n",
            "Rouge 2 :  0.25\n",
            "Rouge L :  0.6\n",
            "b.canadian=(amod)=>conservatives\n",
            "a.conservatives=(nsubj)=>withdraw\n",
            "<BasicElement: conservatives-[nsubj]->withdraw>\n",
            "a.conservatives=(nsubj)=>gain\n",
            "<BasicElement: conservatives-[nsubj]->gain>\n",
            "a.momentum=(dobj)=>gain\n",
            "<BasicElement: momentum-[dobj]->gain>\n",
            "b.ruling=(amod)=>liberals\n",
            "Rouge 1 :  0.3636363636363636\n",
            "Rouge 2 :  0\n",
            "Rouge L :  0.3636363636363636\n",
            "b.dutch=(compound)=>bank\n",
            "a.bank=(nsubj)=>gives\n",
            "<BasicElement: bank-[nsubj]->give>\n",
            "Rouge 1 :  0.28571428571428575\n",
            "Rouge 2 :  0\n",
            "Rouge L :  0.28571428571428575\n",
            "b.new=(amod)=>up\n",
            "a.bills=(nsubj)=>shake\n",
            "<BasicElement: bills-[nsubj]->shake>\n",
            "b.front=(amod)=>office\n",
            "a.office=(dobj)=>shake\n",
            "<BasicElement: office-[dobj]->shake>\n",
            "Rouge 1 :  0.28571428571428575\n",
            "Rouge 2 :  0\n",
            "Rouge L :  0.28571428571428575\n",
            "a.surrender=(compound)=>keane\n",
            "<BasicElement: surrender-[compound]->keane>\n",
            "a.winner=(dobj)=>spurs\n",
            "<BasicElement: winner-[dobj]->spur>\n",
            "b.compound=(amod)=>woes\n",
            "a.woes=(dobj)=>spurs\n",
            "<BasicElement: woes-[dobj]->spur>\n",
            "Rouge 1 :  0.22222222222222224\n",
            "Rouge 2 :  0\n",
            "Rouge L :  0.22222222222222224\n",
            "a.dollar=(nsubj)=>falls\n",
            "<BasicElement: dollar-[nsubj]->fall>\n",
            "a.outlook=(nsubj)=>sends\n",
            "<BasicElement: outlook-[nsubj]->send>\n",
            "a.dollar=(dobj)=>sends\n",
            "<BasicElement: dollar-[dobj]->send>\n",
            "Rouge 1 :  0.5454545454545454\n",
            "Rouge 2 :  0.2222222222222222\n",
            "Rouge L :  0.5454545454545454\n",
            "b.honors=(dobj)=>unk\n",
            "b.top=(amod)=>groups\n",
            "a.groups=(nsubj)=>pick\n",
            "<BasicElement: groups-[nsubj]->pick>\n",
            "a.nominees=(dobj)=>pick\n",
            "<BasicElement: nominees-[dobj]->pick>\n",
            "Rouge 1 :  0\n",
            "Rouge 2 :  0\n",
            "Rouge L :  0\n",
            "b.signs=(pobj)=>unk\n",
            "a.pirates=(nsubj)=>ink\n",
            "<BasicElement: pirates-[nsubj]->ink>\n",
            "Rouge 1 :  0.1818181818181818\n",
            "Rouge 2 :  0\n",
            "Rouge L :  0.1818181818181818\n",
            "b.first=(amod)=>lady\n",
            "b.half=(amod)=>year\n",
            "a.bush=(nsubj)=>says\n",
            "<BasicElement: bush-[nsubj]->say>\n",
            "b.israelis=(compound)=>concern\n",
            "a.concern=(dobj)=>shares\n",
            "<BasicElement: concern-[dobj]->share>\n",
            "Rouge 1 :  0.15384615384615385\n",
            "Rouge 2 :  0\n",
            "Rouge L :  0.15384615384615385\n",
            "b.prime=(amod)=>minister\n",
            "a.minister=(compound)=>resigns\n",
            "<BasicElement: minister-[compound]->resign>\n",
            "b.resigns=(conj)=>faso\n",
            "Rouge 1 :  0.6\n",
            "Rouge 2 :  0.25\n",
            "Rouge L :  0.6\n",
            "b.french=(amod)=>court\n",
            "a.court=(nsubj)=>seeks\n",
            "<BasicElement: court-[nsubj]->seek>\n",
            "a.lid=(dobj)=>bar\n",
            "<BasicElement: lid-[dobj]->bar>\n",
            "a.man=(nsubj)=>seeks\n",
            "<BasicElement: man-[nsubj]->seek>\n",
            "b.stay=(amod)=>order\n",
            "a.order=(dobj)=>seeks\n",
            "<BasicElement: order-[dobj]->seek>\n",
            "Rouge 1 :  0.16666666666666666\n",
            "Rouge 2 :  0\n",
            "Rouge L :  0.16666666666666666\n",
            "b.ministerial=(amod)=>session\n",
            "a.session=(dobj)=>hold\n",
            "<BasicElement: session-[dobj]->hold>\n",
            "b.great=(amod)=>region\n",
            "a.council=(nsubj)=>hold\n",
            "<BasicElement: council-[nsubj]->hold>\n",
            "b.ministerial=(amod)=>session\n",
            "a.session=(dobj)=>hold\n",
            "<BasicElement: session-[dobj]->hold>\n",
            "b.great=(amod)=>lakes\n",
            "Rouge 1 :  0.7999999999999999\n",
            "Rouge 2 :  0.7692307692307692\n",
            "Rouge L :  0.7999999999999999\n",
            "b.israeli=(amod)=>leaders\n",
            "a.leaders=(nsubj)=>meet\n",
            "<BasicElement: leaders-[nsubj]->meet>\n",
            "a.pm=(nsubj)=>undergoes\n",
            "<BasicElement: pm-[nsubj]->undergoes>\n",
            "a.surgery=(dobj)=>undergoes\n",
            "<BasicElement: surgery-[dobj]->undergoes>\n",
            "b.israeli=(amod)=>leaders\n",
            "a.leaders=(nsubj)=>unite\n",
            "<BasicElement: leaders-[nsubj]->unite>\n",
            "a.sharon=(dobj)=>ailing\n",
            "<BasicElement: sharon-[dobj]->ailing>\n",
            "Rouge 1 :  0.42857142857142855\n",
            "Rouge 2 :  0.16666666666666666\n",
            "Rouge L :  0.42857142857142855\n",
            "a.operation=(dobj)=>undergoes\n",
            "<BasicElement: operation-[dobj]->undergoes>\n",
            "b.big=(amod)=>stroke\n",
            "Rouge 1 :  0.2222222222222222\n",
            "Rouge 2 :  0\n",
            "Rouge L :  0.2222222222222222\n",
            "a.who=(nsubj)=>lose\n",
            "<BasicElement: who-[nsubj]->lose>\n",
            "a.deportation=(dobj)=>lose\n",
            "<BasicElement: deportation-[dobj]->lose>\n",
            "b.human=(amod)=>trafficking\n",
            "a.victims=(nsubj)=>get\n",
            "<BasicElement: victims-[nsubj]->get>\n",
            "Rouge 1 :  0.25\n",
            "Rouge 2 :  0\n",
            "Rouge L :  0.25\n",
            "b.australian=(amod)=>open\n",
            "Rouge 1 :  0.2\n",
            "Rouge 2 :  0\n",
            "Rouge L :  0.2\n",
            "a.unk=(nsubj)=>retires\n",
            "<BasicElement: unk-[nsubj]->retire>\n",
            "b.odd=(amod)=>man\n",
            "a.devils=(nsubj)=>welcome\n",
            "<BasicElement: devils-[nsubj]->welcome>\n",
            "a.elias=(dobj)=>welcome\n",
            "<BasicElement: elias-[dobj]->welcome>\n",
            "Rouge 1 :  0\n",
            "Rouge 2 :  0\n",
            "Rouge L :  0\n",
            "b.israeli=(amod)=>minister\n",
            "a.emergency=(compound)=>israel\n",
            "<BasicElement: emergency-[compound]->israel>\n",
            "b.israel=(dobj)=>chair\n",
            "Rouge 1 :  0.30769230769230765\n",
            "Rouge 2 :  0.1818181818181818\n",
            "Rouge L :  0.30769230769230765\n",
            "a.star=(nsubj)=>turns\n",
            "<BasicElement: star-[nsubj]->turn>\n",
            "a.credit=(dobj)=>driving\n",
            "<BasicElement: credit-[dobj]->drive>\n",
            "a.actor=(nmod)=>nolte\n",
            "<BasicElement: actor-[nmod]->nolte>\n",
            "a.nick=(compound)=>nolte\n",
            "<BasicElement: nick-[compound]->nolte>\n",
            "a.unk=(advcl)=>ends\n",
            "<BasicElement: unk-[advcl]->end>\n",
            "Rouge 1 :  0\n",
            "Rouge 2 :  0\n",
            "Rouge L :  0\n",
            "b.south=(amod)=>korea\n",
            "a.korea=(nsubj)=>tops\n",
            "<BasicElement: korea-[nsubj]->top>\n",
            "a.confidence=(compound)=>tops\n",
            "<BasicElement: confidence-[compound]->top>\n",
            "a.percent=(npadvmod)=>tops\n",
            "<BasicElement: percent-[npadvmod]->top>\n",
            "a.skorea=(nsubj)=>tops\n",
            "<BasicElement: skorea-[nsubj]->top>\n",
            "a.confidence=(nsubj)=>tops\n",
            "<BasicElement: confidence-[nsubj]->top>\n",
            "Rouge 1 :  0.5454545454545454\n",
            "Rouge 2 :  0.4444444444444445\n",
            "Rouge L :  0.5454545454545454\n",
            "a.sheen=(dobj)=>charlie\n",
            "<BasicElement: sheen-[dobj]->charlie>\n",
            "a.divorce=(dobj)=>blackmail\n",
            "<BasicElement: divorce-[dobj]->blackmail>\n",
            "b.push=(dobj)=>charlie\n",
            "Rouge 1 :  0.4615384615384615\n",
            "Rouge 2 :  0.1818181818181818\n",
            "Rouge L :  0.4615384615384615\n",
            "b.hong=(compound)=>kong\n",
            "a.gold=(nsubj)=>opens\n",
            "<BasicElement: gold-[nsubj]->open>\n",
            "b.hong=(compound)=>kong\n",
            "a.gold=(nsubj)=>opens\n",
            "<BasicElement: gold-[nsubj]->open>\n",
            "Rouge 1 :  1.0\n",
            "Rouge 2 :  1.0\n",
            "Rouge L :  1.0\n",
            "a.shares=(nsubj)=>rise\n",
            "<BasicElement: shares-[nsubj]->rise>\n",
            "a.percent=(npadvmod)=>rise\n",
            "<BasicElement: percent-[npadvmod]->rise>\n",
            "a.shares=(nsubj)=>rise\n",
            "<BasicElement: shares-[nsubj]->rise>\n",
            "a.percent=(npadvmod)=>rise\n",
            "<BasicElement: percent-[npadvmod]->rise>\n",
            "Rouge 1 :  1.0\n",
            "Rouge 2 :  1.0\n",
            "Rouge L :  1.0\n",
            "b.top=(amod)=>cop\n",
            "a.cop=(nsubj)=>fined\n",
            "<BasicElement: cop-[nsubj]->fin>\n",
            "b.british=(amod)=>police\n",
            "a.police=(nsubj)=>seek\n",
            "<BasicElement: police-[nsubj]->seek>\n",
            "a.moss=(dobj)=>arrest\n",
            "<BasicElement: moss-[dobj]->arrest>\n",
            "Rouge 1 :  0.18181818181818182\n",
            "Rouge 2 :  0\n",
            "Rouge L :  0.18181818181818182\n",
            "b.key=(amod)=>facts\n",
            "b.hemorrhagic=(amod)=>stroke\n",
            "Rouge 1 :  0\n",
            "Rouge 2 :  0\n",
            "Rouge L :  0\n",
            "b.hong=(compound)=>kong\n",
            "a.shares=(nsubj)=>open\n",
            "<BasicElement: shares-[nsubj]->open>\n",
            "b.hong=(compound)=>kong\n",
            "a.shares=(nsubj)=>open\n",
            "<BasicElement: shares-[nsubj]->open>\n",
            "a.worries=(advcl)=>open\n",
            "<BasicElement: worries-[advcl]->open>\n",
            "a.ease=(conj)=>open\n",
            "<BasicElement: ease-[conj]->open>\n",
            "Rouge 1 :  0.7692307692307693\n",
            "Rouge 2 :  0.7272727272727273\n",
            "Rouge L :  0.7692307692307693\n",
            "b.afp=(compound)=>schedule\n",
            "a.inter=(amod)=>korean\n",
            "<BasicElement: inter-[amod]->korean>\n",
            "a.trade=(compound)=>doubles\n",
            "<BasicElement: trade-[compound]->double>\n",
            "Rouge 1 :  0\n",
            "Rouge 2 :  0\n",
            "Rouge L :  0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkB1DfkQmnuC",
        "colab_type": "code",
        "outputId": "bc501c58-939b-4955-f1f8-03fdbb7c1bb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(summary_array)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeY_O38Bm4TL",
        "colab_type": "code",
        "outputId": "fefa2112-f2ab-487f-a503-d94249b9ac10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(summary_array[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "us olympic champion retires\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNh6nUQ6m6tg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checking = \"The Lion Air flight JT 610 had been carrying 189 people, including three children, when it disappeared from radar during a short flight from Jakarta to Pangkal Pinang on the Indonesian island of Bangka, according to Basarnas, Indonesia's national search and rescue agency.\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9k8_fMssnE4X",
        "colab_type": "code",
        "outputId": "a417bee6-41d4-416c-9d74-2aa56e2beb47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(nltk.word_tokenize(checking))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['The', 'Lion', 'Air', 'flight', 'JT', '610', 'had', 'been', 'carrying', '189', 'people', ',', 'including', 'three', 'children', ',', 'when', 'it', 'disappeared', 'from', 'radar', 'during', 'a', 'short', 'flight', 'from', 'Jakarta', 'to', 'Pangkal', 'Pinang', 'on', 'the', 'Indonesian', 'island', 'of', 'Bangka', ',', 'according', 'to', 'Basarnas', ',', 'Indonesia', \"'s\", 'national', 'search', 'and', 'rescue', 'agency', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyYOHk1lJUM9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}